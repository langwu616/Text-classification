{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_457",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxrhB-axNwCk",
        "colab_type": "code",
        "outputId": "1772323c-d8a1-445b-ebad-deaa2e98a4cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import zipfile\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import datetime\n",
        "\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "emotions = [\"anger\", \"anticipation\", \"disgust\", \"fear\", \"joy\", \"love\",\n",
        "            \"optimism\", \"pessimism\", \"sadness\", \"surprise\", \"trust\"]\n",
        "emotion_to_int = {\"0\": 0, \"1\": 1, \"NONE\": -1}\n",
        "%load_ext tensorboard\n",
        "callbacks=[EarlyStopping(patience=3, restore_best_weights=True)]\n",
        "file1='/content/2018-E-c-En-train.txt'\n",
        "file2='/content/2018-E-c-En-dev.txt'\n",
        "train_data=pd.read_csv(file1, sep=\"\\t\", header=0)\n",
        "dev_data=pd.read_csv(file2, sep=\"\\t\", header=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF6lZ8ogOb-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "max_length = 35\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "num_sentences =len(train_data)\n",
        "corpus_train=[]\n",
        "for i in range(num_sentences):\n",
        "  corpus_train.append([train_data.iloc[i,1],train_data.iloc[i,2:13]])\n",
        "corpus_val=[]\n",
        "num_val_sentences=len(dev_data)\n",
        "for i in range(num_val_sentences):\n",
        "  corpus_val.append([dev_data.iloc[i,1],dev_data.iloc[i,2:13]])\n",
        "\n",
        "val_sen=[]\n",
        "val_lab=[] \n",
        "# random.shuffle(corpus_train)\n",
        "for x in range(num_val_sentences):\n",
        "    val_sen.append(corpus_val[x][0])\n",
        "    val_lab.append(corpus_val[x][1])\n",
        "    \n",
        "sentences=[]\n",
        "labels=[]\n",
        "for x in range(num_sentences):\n",
        "    sentences.append(corpus_train[x][0])\n",
        "    labels.append(corpus_train[x][1])\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "training_sequences = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val_seq=tokenizer.texts_to_sequences(val_sen)\n",
        "val_sequences=pad_sequences(val_seq,maxlen=max_length,padding=padding_type, truncating=trunc_type)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPVX5WF56LJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRUPd7RpOJsG",
        "colab_type": "code",
        "outputId": "946a5d2a-93ab-47cb-9f49-8d565987d94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# Note this is the 100 dimension version of GloVe from Stanford\n",
        "# I unzipped to make this notebook easier\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n",
        "    -O /tmp/glove.6B.100d.txt\n",
        "embeddings_index = {};\n",
        "with open('/tmp/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector;"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-04 05:07:52--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 2a00:1450:4013:c05::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347116733 (331M) [text/plain]\n",
            "Saving to: ‘/tmp/glove.6B.100d.txt’\n",
            "\n",
            "/tmp/glove.6B.100d. 100%[===================>] 331.04M  72.5MB/s    in 4.6s    \n",
            "\n",
            "2019-12-04 05:07:57 (72.5 MB/s) - ‘/tmp/glove.6B.100d.txt’ saved [347116733/347116733]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slekvFZM4vFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label=np.array(labels)\n",
        "val_labs=np.array(val_lab)\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "tf.keras.layers.Dropout(.4),\n",
        "tf.keras.layers.Conv1D(6400, kernel_size=3, activation='relu',padding='valid'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=3),\n",
        "\n",
        "tf.keras.layers.Conv1D(6400, kernel_size=3, activation='relu',padding='valid'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=3),\n",
        "tf.keras.layers.GlobalAveragePooling1D(),\n",
        "tf.keras.layers.Dense(12800, activation='relu'),\n",
        "tf.keras.layers.Dropout(.40),\n",
        "tf.keras.layers.Dense(11, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "#model.summary()\n",
        "num_epochs =150\n",
        "history = model.fit(training_sequences, label,batch_size=64, epochs=num_epochs, validation_data=(val_sequences, val_labs), verbose=1,callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcnSm-AzJMl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVlZzfV0JM6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ9opy3D64eZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_prediction=model.predict(val_sequences)\n",
        "dev_predictions = np.zeros(dev_prediction.shape)\n",
        "dev_predictions[dev_prediction>0.4] = 1\n",
        "dev_predictions=pd.DataFrame(dev_predictions)\n",
        "dev_predictions.columns=emotions\n",
        "\n",
        "print(\"accuracy: {:.3f}\".format(sklearn.metrics.jaccard_similarity_score(\n",
        "    dev_data[emotions], dev_predictions[emotions])))\n",
        "dev_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPQYCXi6fVB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTvyzjgefnsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-TVtBepRyr-",
        "colab_type": "code",
        "outputId": "39c6fce5-7b9e-4be5-cf15-40ffe906042b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        }
      },
      "source": [
        "label=np.array(labels)\n",
        "val_labs=np.array(val_lab)\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "# tf.keras.layers.Conv1D(256, kernel_size=3, padding='same', activation='relu'),\n",
        "# tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "# tf.keras.layers.BatchNormalization(axis=-1),\n",
        "tf.keras.layers.Conv1D(512, kernel_size=3, padding='same', activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "tf.keras.layers.Dropout(.15),\n",
        "tf.keras.layers.Conv1D(256, kernel_size=3, padding='same', activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "tf.keras.layers.Dropout(.15),\n",
        "\n",
        "tf.keras.layers.Conv1D(64, kernel_size=3, padding='same', activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "\n",
        "tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64,dropout=.2,recurrent_dropout=.2)),\n",
        "tf.keras.layers.Dense(11, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "#model.summary()\n",
        "num_epochs =50\n",
        "history = model.fit(training_sequences, label,batch_size=64, epochs=num_epochs, validation_data=(val_sequences, val_labs), verbose=1,callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 6838 samples, validate on 886 samples\n",
            "Epoch 1/50\n",
            "6838/6838 [==============================] - 8s 1ms/sample - loss: 0.4838 - acc: 0.7790 - val_loss: 0.4737 - val_acc: 0.7787\n",
            "Epoch 2/50\n",
            "6838/6838 [==============================] - 2s 287us/sample - loss: 0.4326 - acc: 0.8079 - val_loss: 0.3870 - val_acc: 0.8332\n",
            "Epoch 3/50\n",
            "6838/6838 [==============================] - 2s 293us/sample - loss: 0.3756 - acc: 0.8394 - val_loss: 0.3673 - val_acc: 0.8447\n",
            "Epoch 4/50\n",
            "6838/6838 [==============================] - 2s 291us/sample - loss: 0.3480 - acc: 0.8550 - val_loss: 0.3571 - val_acc: 0.8510\n",
            "Epoch 5/50\n",
            "6838/6838 [==============================] - 2s 290us/sample - loss: 0.3195 - acc: 0.8692 - val_loss: 0.3563 - val_acc: 0.8523\n",
            "Epoch 6/50\n",
            "6838/6838 [==============================] - 2s 296us/sample - loss: 0.2984 - acc: 0.8779 - val_loss: 0.3527 - val_acc: 0.8558\n",
            "Epoch 7/50\n",
            "6838/6838 [==============================] - 2s 297us/sample - loss: 0.2777 - acc: 0.8873 - val_loss: 0.3690 - val_acc: 0.8478\n",
            "Epoch 8/50\n",
            "6838/6838 [==============================] - 2s 290us/sample - loss: 0.2585 - acc: 0.8951 - val_loss: 0.3621 - val_acc: 0.8547\n",
            "Epoch 9/50\n",
            "6838/6838 [==============================] - 2s 293us/sample - loss: 0.2422 - acc: 0.9025 - val_loss: 0.3685 - val_acc: 0.8531\n",
            "Epoch 10/50\n",
            "6838/6838 [==============================] - 2s 296us/sample - loss: 0.2244 - acc: 0.9100 - val_loss: 0.3836 - val_acc: 0.8509\n",
            "Epoch 11/50\n",
            "6838/6838 [==============================] - 2s 318us/sample - loss: 0.2090 - acc: 0.9156 - val_loss: 0.3857 - val_acc: 0.8501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZWgcGiWSzab",
        "colab_type": "code",
        "outputId": "8cbd112a-a48d-4307-f7e2-3dad0d64fba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "dev_prediction=model.predict(val_sequences)\n",
        "dev_predictions = np.zeros(dev_prediction.shape)\n",
        "dev_predictions[dev_prediction>0.33] = 1\n",
        "dev_predictions=pd.DataFrame(dev_predictions)\n",
        "dev_predictions.columns=emotions\n",
        "\n",
        "print(\"accuracy: {:.3f}\".format(sklearn.metrics.jaccard_similarity_score(\n",
        "    dev_data[emotions], dev_predictions[emotions])))\n",
        "dev_predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.519\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  'and multiclass classification tasks.', DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>anger</th>\n",
              "      <th>anticipation</th>\n",
              "      <th>disgust</th>\n",
              "      <th>fear</th>\n",
              "      <th>joy</th>\n",
              "      <th>love</th>\n",
              "      <th>optimism</th>\n",
              "      <th>pessimism</th>\n",
              "      <th>sadness</th>\n",
              "      <th>surprise</th>\n",
              "      <th>trust</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>884</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>886 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     anger  anticipation  disgust  fear  ...  pessimism  sadness  surprise  trust\n",
              "0      1.0           0.0      1.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "1      0.0           0.0      0.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "2      1.0           0.0      1.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "3      0.0           1.0      0.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "4      0.0           0.0      0.0   1.0  ...        0.0      0.0       0.0    0.0\n",
              "..     ...           ...      ...   ...  ...        ...      ...       ...    ...\n",
              "881    1.0           0.0      1.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "882    0.0           0.0      0.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "883    0.0           0.0      1.0   1.0  ...        0.0      1.0       0.0    0.0\n",
              "884    0.0           0.0      0.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "885    0.0           0.0      0.0   0.0  ...        0.0      0.0       0.0    0.0\n",
              "\n",
              "[886 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U88ZxEXx9sFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_predictions[:] = dev_predictions[:].astype(int)\n",
        "dev_res_data=pd.read_csv(file2, sep=\"\\t\", header=0)\n",
        "dev_res_data.iloc[:,3:]=dev_predictions\n",
        "dev_res_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXfR4NHWCft9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dev_predictions['ID']=dev_data.ID\n",
        "# dev_predictions['Tweet']=dev_data.Tweet\n",
        "# dev_predictions\n",
        "# cols = dev_predictions.columns.tolist()\n",
        "# cols = cols[-1:] + cols[:-1]\n",
        "# cols = cols[-1:] + cols[:-1]\n",
        "# cols\n",
        "# dev_predictions = dev_predictions[cols]\n",
        "# dev_predictions = dev_predictions[cols]\n",
        "# dev_predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8XOT_nS5p6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_res_data.to_csv(\"E-C_en_pred.txt\", sep=\"\\t\", index=False)\n",
        "with zipfile.ZipFile('submission.zip', mode='w') as submission_zip:\n",
        "    submission_zip.write(\"E-C_en_pred.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_l0ktpArJWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%tensorboard --logdir logs/fit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVa2wnEzSOUI",
        "colab_type": "code",
        "outputId": "d954e278-b809-484b-af7b-c94f66888755",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "random.seed(123)\n",
        "label=np.array(labels)\n",
        "val_labs=np.array(val_lab)\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n",
        "tf.keras.layers.Conv1D(128,kernel_size=3,padding='same',activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "tf.keras.layers.Dropout(0.2),\n",
        "# tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128,dropout=.15,recurrent_dropout=.15,return_sequences=True)),\n",
        "tf.keras.layers.GRU(32,dropout=.2,recurrent_dropout=.15),\n",
        "tf.keras.layers.Dropout(0.2),\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dense(128,activation='relu'),\n",
        "tf.keras.layers.Dropout(0.2),\n",
        "tf.keras.layers.Dense(11, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001),metrics=['acc'])\n",
        "#model.summary()\n",
        "\n",
        "num_epochs =50\n",
        "history = model.fit(training_sequences, label,batch_size=128, epochs=num_epochs, validation_data=(val_sequences, val_labs), verbose=1,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6838 samples, validate on 886 samples\n",
            "Epoch 1/50\n",
            "6838/6838 [==============================] - 6s 894us/sample - loss: 0.5249 - acc: 0.7756 - val_loss: 0.4769 - val_acc: 0.7783\n",
            "Epoch 2/50\n",
            "6838/6838 [==============================] - 2s 243us/sample - loss: 0.4752 - acc: 0.7863 - val_loss: 0.4744 - val_acc: 0.7783\n",
            "Epoch 3/50\n",
            "6838/6838 [==============================] - 2s 242us/sample - loss: 0.4749 - acc: 0.7864 - val_loss: 0.4761 - val_acc: 0.7783\n",
            "Epoch 4/50\n",
            "6838/6838 [==============================] - 2s 236us/sample - loss: 0.4709 - acc: 0.7867 - val_loss: 0.4624 - val_acc: 0.7784\n",
            "Epoch 5/50\n",
            "6838/6838 [==============================] - 2s 238us/sample - loss: 0.4280 - acc: 0.8102 - val_loss: 0.3922 - val_acc: 0.8275\n",
            "Epoch 6/50\n",
            "6838/6838 [==============================] - 2s 240us/sample - loss: 0.3833 - acc: 0.8340 - val_loss: 0.3889 - val_acc: 0.8304\n",
            "Epoch 7/50\n",
            "6838/6838 [==============================] - 2s 240us/sample - loss: 0.3581 - acc: 0.8469 - val_loss: 0.3878 - val_acc: 0.8380\n",
            "Epoch 8/50\n",
            "6838/6838 [==============================] - 2s 239us/sample - loss: 0.3356 - acc: 0.8590 - val_loss: 0.3943 - val_acc: 0.8368\n",
            "Epoch 9/50\n",
            "6838/6838 [==============================] - 2s 237us/sample - loss: 0.3187 - acc: 0.8671 - val_loss: 0.3994 - val_acc: 0.8295\n",
            "Epoch 10/50\n",
            "6838/6838 [==============================] - 2s 237us/sample - loss: 0.3049 - acc: 0.8715 - val_loss: 0.3922 - val_acc: 0.8369\n",
            "Epoch 11/50\n",
            "6838/6838 [==============================] - 2s 239us/sample - loss: 0.2912 - acc: 0.8783 - val_loss: 0.4092 - val_acc: 0.8320\n",
            "Epoch 12/50\n",
            "6838/6838 [==============================] - 4s 548us/sample - loss: 0.2807 - acc: 0.8813 - val_loss: 0.4062 - val_acc: 0.8355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jINMw_dAS0MP",
        "colab_type": "code",
        "outputId": "132cac1c-e5c4-474e-fe29-cbe4ca309384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "dev_prediction=model.predict(val_sequences)\n",
        "dev_predictions = np.zeros(dev_prediction.shape)\n",
        "dev_predictions[dev_prediction>0.35] = 1\n",
        "dev_predictions=pd.DataFrame(dev_predictions)\n",
        "dev_predictions.columns=emotions\n",
        "\n",
        "print(\"accuracy: {:.3f}\".format(sklearn.metrics.jaccard_similarity_score(\n",
        "    dev_data[emotions], dev_predictions[emotions])))\n",
        "dev_predictions[:] = dev_predictions[:].astype(int)\n",
        "dev_res_data=pd.read_csv(file2, sep=\"\\t\", header=0)\n",
        "dev_res_data.iloc[:,3:]=dev_predictions\n",
        "dev_res_data.to_csv(\"E-C_en_pred.txt\", sep=\"\\t\", index=False)\n",
        "with zipfile.ZipFile('submission.zip', mode='w') as submission_zip:\n",
        "    submission_zip.write(\"E-C_en_pred.txt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  'and multiclass classification tasks.', DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-q0g4nxf8s3",
        "colab_type": "code",
        "outputId": "bd912582-bc80-45c3-da95-9ae79b6c363e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        " model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "    tf.keras.layers.Conv1D(256, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Dropout(.15),\n",
        "    tf.keras.layers.Conv1D(128, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Conv1D(128, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32,dropout=.3,recurrent_dropout=.1)),\n",
        "    tf.keras.layers.Dense(11, activation='sigmoid')\n",
        "    ])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])\n",
        "#model.summary()\n",
        "num_epochs =50\n",
        "history = model.fit(training_sequences, label,batch_size=128, epochs=num_epochs, validation_data=(val_sequences, val_labs), verbose=1,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6838 samples, validate on 886 samples\n",
            "Epoch 1/50\n",
            "6838/6838 [==============================] - 7s 1ms/sample - loss: 0.4933 - acc: 0.7726 - val_loss: 0.4751 - val_acc: 0.7783\n",
            "Epoch 2/50\n",
            "6838/6838 [==============================] - 1s 159us/sample - loss: 0.4709 - acc: 0.7862 - val_loss: 0.4685 - val_acc: 0.7786\n",
            "Epoch 3/50\n",
            "6838/6838 [==============================] - 1s 152us/sample - loss: 0.4322 - acc: 0.8078 - val_loss: 0.3984 - val_acc: 0.8251\n",
            "Epoch 4/50\n",
            "6838/6838 [==============================] - 1s 155us/sample - loss: 0.3961 - acc: 0.8274 - val_loss: 0.3836 - val_acc: 0.8358\n",
            "Epoch 5/50\n",
            "6838/6838 [==============================] - 1s 159us/sample - loss: 0.3798 - acc: 0.8366 - val_loss: 0.3737 - val_acc: 0.8431\n",
            "Epoch 6/50\n",
            "6838/6838 [==============================] - 1s 164us/sample - loss: 0.3570 - acc: 0.8496 - val_loss: 0.3706 - val_acc: 0.8422\n",
            "Epoch 7/50\n",
            "6838/6838 [==============================] - 1s 160us/sample - loss: 0.3365 - acc: 0.8589 - val_loss: 0.3676 - val_acc: 0.8486\n",
            "Epoch 8/50\n",
            "6838/6838 [==============================] - 1s 159us/sample - loss: 0.3181 - acc: 0.8671 - val_loss: 0.3736 - val_acc: 0.8503\n",
            "Epoch 9/50\n",
            "6838/6838 [==============================] - 1s 159us/sample - loss: 0.3051 - acc: 0.8747 - val_loss: 0.3638 - val_acc: 0.8513\n",
            "Epoch 10/50\n",
            "6838/6838 [==============================] - 1s 156us/sample - loss: 0.2891 - acc: 0.8828 - val_loss: 0.3804 - val_acc: 0.8473\n",
            "Epoch 11/50\n",
            "6838/6838 [==============================] - 1s 157us/sample - loss: 0.2823 - acc: 0.8854 - val_loss: 0.3564 - val_acc: 0.8548\n",
            "Epoch 12/50\n",
            "6838/6838 [==============================] - 1s 157us/sample - loss: 0.2654 - acc: 0.8932 - val_loss: 0.3640 - val_acc: 0.8545\n",
            "Epoch 13/50\n",
            "6838/6838 [==============================] - 1s 153us/sample - loss: 0.2567 - acc: 0.8970 - val_loss: 0.3724 - val_acc: 0.8488\n",
            "Epoch 14/50\n",
            "6838/6838 [==============================] - 1s 157us/sample - loss: 0.2446 - acc: 0.9012 - val_loss: 0.3675 - val_acc: 0.8560\n",
            "Epoch 15/50\n",
            "6838/6838 [==============================] - 1s 154us/sample - loss: 0.2380 - acc: 0.9045 - val_loss: 0.3767 - val_acc: 0.8508\n",
            "Epoch 16/50\n",
            "6838/6838 [==============================] - 4s 550us/sample - loss: 0.2297 - acc: 0.9073 - val_loss: 0.3837 - val_acc: 0.8455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M01BkMf-k_aT",
        "colab_type": "code",
        "outputId": "a8f28e46-856f-4fea-b84c-8a4df2be5d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "dev_prediction=model.predict(val_sequences)\n",
        "dev_predictions = np.zeros(dev_prediction.shape)\n",
        "dev_predictions[dev_prediction>0.32] = 1\n",
        "dev_predictions=pd.DataFrame(dev_predictions)\n",
        "dev_predictions.columns=emotions\n",
        "\n",
        "print(\"accuracy: {:.3f}\".format(sklearn.metrics.jaccard_similarity_score(\n",
        "    dev_data[emotions], dev_predictions[emotions])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  'and multiclass classification tasks.', DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xxHZN_CcfN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_predictions[:] = dev_predictions[:].astype(int)\n",
        "dev_res_data=pd.read_csv(file2, sep=\"\\t\", header=0)\n",
        "dev_res_data.iloc[:,3:]=dev_predictions\n",
        "dev_res_data.to_csv(\"E-C_en_pred.txt\", sep=\"\\t\", index=False)\n",
        "with zipfile.ZipFile('submission.zip', mode='w') as submission_zip:\n",
        "    submission_zip.write(\"E-C_en_pred.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ_7jG7NOgyq",
        "colab_type": "code",
        "outputId": "40a39d6b-6195-443e-c0ea-f8f1a966dd23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "random.seed(123)\n",
        "label=np.array(labels)\n",
        "val_labs=np.array(val_lab)\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "tf.keras.layers.Conv1D(160,kernel_size=3,padding='same',activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(120,dropout=.2,recurrent_dropout=.2)),\n",
        "#tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(8),input_shape=(10,16)),\n",
        "#tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dense(80,activation='relu'),\n",
        "tf.keras.layers.Dropout(0.15),\n",
        "tf.keras.layers.Dense(11, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001),metrics=['acc'])\n",
        "#model.summary()\n",
        "\n",
        "num_epochs =50\n",
        "history = model.fit(training_sequences, label,batch_size=128, epochs=num_epochs, validation_data=(val_sequences, val_labs), verbose=1,callbacks=callbacks)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6838 samples, validate on 886 samples\n",
            "Epoch 1/50\n",
            "6838/6838 [==============================] - 12s 2ms/sample - loss: 0.4971 - acc: 0.7677 - val_loss: 0.4737 - val_acc: 0.7783\n",
            "Epoch 2/50\n",
            "6838/6838 [==============================] - 3s 497us/sample - loss: 0.4448 - acc: 0.8008 - val_loss: 0.3972 - val_acc: 0.8245\n",
            "Epoch 3/50\n",
            "6838/6838 [==============================] - 3s 499us/sample - loss: 0.4082 - acc: 0.8211 - val_loss: 0.3862 - val_acc: 0.8306\n",
            "Epoch 4/50\n",
            "6838/6838 [==============================] - 3s 488us/sample - loss: 0.3936 - acc: 0.8278 - val_loss: 0.3781 - val_acc: 0.8349\n",
            "Epoch 5/50\n",
            "6838/6838 [==============================] - 3s 486us/sample - loss: 0.3747 - acc: 0.8391 - val_loss: 0.3696 - val_acc: 0.8414\n",
            "Epoch 6/50\n",
            "6838/6838 [==============================] - 3s 498us/sample - loss: 0.3589 - acc: 0.8469 - val_loss: 0.3688 - val_acc: 0.8407\n",
            "Epoch 7/50\n",
            "6838/6838 [==============================] - 3s 496us/sample - loss: 0.3470 - acc: 0.8542 - val_loss: 0.3578 - val_acc: 0.8498\n",
            "Epoch 8/50\n",
            "6838/6838 [==============================] - 3s 475us/sample - loss: 0.3352 - acc: 0.8595 - val_loss: 0.3539 - val_acc: 0.8559\n",
            "Epoch 9/50\n",
            "6838/6838 [==============================] - 3s 497us/sample - loss: 0.3227 - acc: 0.8667 - val_loss: 0.3617 - val_acc: 0.8503\n",
            "Epoch 10/50\n",
            "6838/6838 [==============================] - 3s 488us/sample - loss: 0.3165 - acc: 0.8687 - val_loss: 0.3493 - val_acc: 0.8518\n",
            "Epoch 11/50\n",
            "6838/6838 [==============================] - 3s 508us/sample - loss: 0.3072 - acc: 0.8730 - val_loss: 0.3554 - val_acc: 0.8503\n",
            "Epoch 12/50\n",
            "6838/6838 [==============================] - 3s 484us/sample - loss: 0.2998 - acc: 0.8755 - val_loss: 0.3492 - val_acc: 0.8548\n",
            "Epoch 13/50\n",
            "6838/6838 [==============================] - 3s 478us/sample - loss: 0.2912 - acc: 0.8801 - val_loss: 0.3583 - val_acc: 0.8558\n",
            "Epoch 14/50\n",
            "6838/6838 [==============================] - 3s 489us/sample - loss: 0.2856 - acc: 0.8816 - val_loss: 0.3565 - val_acc: 0.8549\n",
            "Epoch 15/50\n",
            "6838/6838 [==============================] - 8s 1ms/sample - loss: 0.2761 - acc: 0.8862 - val_loss: 0.3610 - val_acc: 0.8572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOMxUG7rpftx",
        "colab_type": "code",
        "outputId": "a603f315-af18-49ce-913c-234096ee8ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "dev_prediction=model.predict(val_sequences)\n",
        "dev_predictions = np.zeros(dev_prediction.shape)\n",
        "dev_predictions[dev_prediction>0.345] = 1\n",
        "dev_predictions=pd.DataFrame(dev_predictions)\n",
        "dev_predictions.columns=emotions\n",
        "\n",
        "print(\"accuracy: {:.3f}\".format(sklearn.metrics.jaccard_similarity_score(\n",
        "    dev_data[emotions], dev_predictions[emotions])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  'and multiclass classification tasks.', DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}